options:
  logging: CLOUD_LOGGING_ONLY
  default_logs_bucket_behavior: REGIONAL_USER_OWNED_BUCKET
timeout: "3600s"

steps:
# 0) Point Terraform to your existing state bucket
- id: "ensure tfstate bucket var"
  name: "gcr.io/cloud-builders/gcloud"
  entrypoint: /bin/sh
  args:
    - -c
    - |
      set -eu
      # 🔧 Use your actual bucket
      BUCKET="vikunja-case-tfstate"
      LOCATION="europe-west4"

      # sanity check (no-op if it exists)
      if gsutil ls -b "gs://$$BUCKET" >/dev/null 2>&1; then
        echo "Using existing bucket gs://$$BUCKET"
      else
        echo "Creating bucket gs://$$BUCKET in $$LOCATION ..."
        gcloud storage buckets create "gs://$$BUCKET" \
          --location=$$LOCATION \
          --uniform-bucket-level-access
      fi

      echo "TF_STATE_BUCKET=$$BUCKET" > /workspace/tfbackend.env

# 1) Terraform apply + capture outputs
- id: "terraform apply + outputs"
  name: "hashicorp/terraform:1.9.6"
  entrypoint: /bin/sh
  args:
    - -c
    - |
      set -eu
      . /workspace/tfbackend.env

      # Repo layout: adjust if your TF lives elsewhere
      if [ -d "./infra/terraform" ]; then
        TFDIR="./infra/terraform"
      else
        TFDIR="."
      fi
      echo "Using Terraform directory: $$TFDIR"

      terraform -chdir="$$TFDIR" init -input=false \
        -backend-config="bucket=$$TF_STATE_BUCKET" \
        -backend-config="prefix=vikunja/state"

      terraform -chdir="$$TFDIR" apply -input=false -auto-approve

      CLUSTER_NAME="$(terraform -chdir="$$TFDIR" output -raw gke_autopilot_name || true)"
      INSTANCE_CONN_NAME="$(terraform -chdir="$$TFDIR" output -raw instance_connection_name || true)"

      {
        echo "PROJECT_ID=${PROJECT_ID}"
        echo "CLUSTER_NAME=$$CLUSTER_NAME"
        echo "INSTANCE_CONN_NAME=$$INSTANCE_CONN_NAME"
      } > /workspace/env.out

      echo "---- env.out ----"
      cat /workspace/env.out

# 2) Connect to GKE and deploy with Helm
- id: "kubectl + helm deploy"
  name: "gcr.io/cloud-builders/gcloud"
  entrypoint: /bin/sh
  args:
    - -c
    - |
      set -eu
      . /workspace/env.out

      REGION="$(gcloud container clusters list \
        --project "$PROJECT_ID" \
        --filter="name=$$CLUSTER_NAME" \
        --format="value(location)")"
      if [ -z "$$REGION" ]; then
        echo "❌ Could not find region for cluster $$CLUSTER_NAME"
        exit 1
      fi

      echo "Using cluster $$CLUSTER_NAME in $$REGION (project $PROJECT_ID)"
      gcloud container clusters get-credentials "$$CLUSTER_NAME" --region "$$REGION" --project "$PROJECT_ID"

      # Ensure namespace exists and is Helm-owned
      kubectl get ns vikunja >/dev/null 2>&1 || kubectl create ns vikunja
      kubectl label namespace vikunja app.kubernetes.io/managed-by=Helm --overwrite
      kubectl annotate namespace vikunja meta.helm.sh/release-name=vikunja --overwrite
      kubectl annotate namespace vikunja meta.helm.sh/release-namespace=vikunja --overwrite

      # Secrets
      DB_PASS="$(gcloud secrets versions access latest --secret=vikunja-db-password --project "$PROJECT_ID")"
      kubectl -n vikunja create secret generic vikunja-db-secret \
        --from-literal=VIKUNJA_DB_HOST="$$INSTANCE_CONN_NAME" \
        --from-literal=VIKUNJA_DB_NAME="vikunja" \
        --from-literal=VIKUNJA_DB_USER="vikunja" \
        --from-literal=VIKUNJA_DB_PASSWORD="$$DB_PASS" \
        --dry-run=client -o yaml | kubectl apply -f -

      JWT_SECRET="$(head -c 32 /dev/urandom | base64 | tr -d '\n')"
      kubectl -n vikunja create secret generic vikunja-app-secret \
        --from-literal=VIKUNJA_JWT_SECRET="$$JWT_SECRET" \
        --dry-run=client -o yaml | kubectl apply -f -

      helm version
      helm upgrade --install vikunja ./helm/vikunja \
        --namespace vikunja \
        --create-namespace \
        --wait
