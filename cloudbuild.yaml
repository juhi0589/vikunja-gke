options:
  logging: CLOUD_LOGGING_ONLY
  default_logs_bucket_behavior: REGIONAL_USER_OWNED_BUCKET

timeout: "3600s"

steps:
# 1) Terraform apply and capture values we need
- id: "terraform apply + outputs"
  name: "hashicorp/terraform:1.9.6"
  entrypoint: /bin/sh
  args:
    - -c
    - |
      set -eu

      # Find directory with .tf files (root or first hit within depth 3)
      if find . -maxdepth 1 -name '*.tf' -print -quit | grep -q .; then
        TFDIR="."
      else
        TFDIR="$(dirname "$(find . -mindepth 1 -maxdepth 3 -type f -name '*.tf' ! -path '*/.terraform/*' -print -quit)")"
      fi

      if [ -z "$$TFDIR" ]; then
        echo "❌ No Terraform files found in repo (depth<=3)."
        exit 1
      fi

      echo "Using Terraform directory: $$TFDIR"

      terraform -chdir="$$TFDIR" init -input=false
      terraform -chdir="$$TFDIR" apply -input=false -auto-approve

      # Read Terraform outputs (make optional-safe with '|| true')
      CLUSTER_NAME="$(terraform -chdir="$$TFDIR" output -raw gke_autopilot_name || true)"
      INSTANCE_CONN_NAME="$(terraform -chdir="$$TFDIR" output -raw instance_connection_name || true)"

      # Persist for next step (escape $ so Cloud Build won't pre-substitute)
      {
        echo "PROJECT_ID=${PROJECT_ID}"
        echo "CLUSTER_NAME=$$CLUSTER_NAME"
        echo "INSTANCE_CONN_NAME=$$INSTANCE_CONN_NAME"
      } > /workspace/env.out

      echo "---- env.out ----"
      cat /workspace/env.out

# 2) Connect to GKE and deploy with Helm
- id: "kubectl + helm deploy"
  name: "gcr.io/cloud-builders/gcloud"
  entrypoint: /bin/sh
  args:
    - -c
    - |
      set -eu
      . /workspace/env.out

      # Discover region for the cluster
      REGION="$(gcloud container clusters list \
        --project "$PROJECT_ID" \
        --filter="name=$$CLUSTER_NAME" \
        --format="value(location)")"

      if [ -z "$$REGION" ]; then
        echo "❌ Could not find region for cluster $$CLUSTER_NAME"
        exit 1
      fi

      echo "Using cluster $$CLUSTER_NAME in $$REGION (project $PROJECT_ID)"
      gcloud container clusters get-credentials "$$CLUSTER_NAME" --region "$$REGION" --project "$PROJECT_ID"

      # Ensure namespace exists and is owned by Helm
      kubectl get ns vikunja >/dev/null 2>&1 || kubectl create ns vikunja
      kubectl label namespace vikunja app.kubernetes.io/managed-by=Helm --overwrite
      kubectl annotate namespace vikunja meta.helm.sh/release-name=vikunja --overwrite
      kubectl annotate namespace vikunja meta.helm.sh/release-namespace=vikunja --overwrite

      # Secrets
      DB_PASS="$(gcloud secrets versions access latest --secret=vikunja-db-password --project "$PROJECT_ID")"

      kubectl -n vikunja create secret generic vikunja-db-secret \
        --from-literal=VIKUNJA_DB_HOST="$$INSTANCE_CONN_NAME" \
        --from-literal=VIKUNJA_DB_NAME="vikunja" \
        --from-literal=VIKUNJA_DB_USER="vikunja" \
        --from-literal=VIKUNJA_DB_PASSWORD="$$DB_PASS" \
        --dry-run=client -o yaml | kubectl apply -f -

      JWT_SECRET="$(head -c 32 /dev/urandom | base64 | tr -d '\n')"
      kubectl -n vikunja create secret generic vikunja-app-secret \
        --from-literal=VIKUNJA_JWT_SECRET="$$JWT_SECRET" \
        --dry-run=client -o yaml | kubectl apply -f -

      helm version
      helm upgrade --install vikunja ./helm/vikunja \
        --namespace vikunja \
        --create-namespace \
        --wait
